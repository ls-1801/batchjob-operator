% This file was created with JabRef 2.4.
% Encoding: Cp1252

@inproceedings{da2015survey,
  title     = {Survey on frameworks for distributed computing: Hadoop, spark and storm},
  author    = {da Silva Morais, Telmo},
  booktitle = {Proceedings of the 10th Doctoral Symposium in Informatics Engineering-DSIE},
  volume    = {15},
  year      = {2015}
}

@inproceedings{10.1145/2523616.2523633,
  author    = {Vavilapalli, Vinod Kumar and Murthy, Arun C. and Douglas, Chris and Agarwal, Sharad and Konar, Mahadev and Evans, Robert and Graves, Thomas and Lowe, Jason and Shah, Hitesh and Seth, Siddharth and Saha, Bikas and Curino, Carlo and O'Malley, Owen and Radia, Sanjay and Reed, Benjamin and Baldeschwieler, Eric},
  title     = {Apache Hadoop YARN: Yet Another Resource Negotiator},
  year      = {2013},
  isbn      = {9781450324281},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2523616.2523633},
  doi       = {10.1145/2523616.2523633},
  abstract  = {The initial design of Apache Hadoop [1] was tightly focused on running massive, MapReduce jobs to process a web crawl. For increasingly diverse companies, Hadoop has become the data and computational agor\'{a}---the de facto place where data and computational resources are shared and accessed. This broad adoption and ubiquitous usage has stretched the initial design well beyond its intended target, exposing two key shortcomings: 1) tight coupling of a specific programming model with the resource management infrastructure, forcing developers to abuse the MapReduce programming model, and 2) centralized handling of jobs' control flow, which resulted in endless scalability concerns for the scheduler.In this paper, we summarize the design, development, and current state of deployment of the next generation of Hadoop's compute platform: YARN. The new architecture we introduced decouples the programming model from the resource management infrastructure, and delegates many scheduling functions (e.g., task fault-tolerance) to per-application components. We provide experimental evidence demonstrating the improvements we made, confirm improved efficiency by reporting the experience of running YARN on production environments (including 100% of Yahoo! grids), and confirm the flexibility claims by discussing the porting of several programming frameworks onto YARN viz. Dryad, Giraph, Hoya, Hadoop MapReduce, REEF, Spark, Storm, Tez.},
  booktitle = {Proceedings of the 4th Annual Symposium on Cloud Computing},
  articleno = {5},
  numpages  = {16},
  location  = {Santa Clara, California},
  series    = {SOCC '13}
}

@inproceedings{hindman2011mesos,
  title     = {Mesos: A Platform for $\{$Fine-Grained$\}$ Resource Sharing in the Data Center},
  author    = {Hindman, Benjamin and Konwinski, Andy and Zaharia, Matei and Ghodsi, Ali and Joseph, Anthony D and Katz, Randy and Shenker, Scott and Stoica, Ion},
  booktitle = {8th USENIX Symposium on Networked Systems Design and Implementation (NSDI 11)},
  year      = {2011}
}

@inproceedings{1558641,
  author    = {Capit, N. and Da Costa, G. and Georgiou, Y. and Huard, G. and Martin, C. and Mounie, G. and Neyron, P. and Richard, O.},
  booktitle = {CCGrid 2005. IEEE International Symposium on Cluster Computing and the Grid, 2005.},
  title     = {A batch scheduler with high level components},
  year      = {2005},
  volume    = {2},
  number    = {},
  pages     = {776-783 Vol. 2},
  doi       = {10.1109/CCGRID.2005.1558641}
}

@inproceedings{zaharia2010spark,
  title={Spark: Cluster computing with working sets},
  author={Zaharia, Matei and Chowdhury, Mosharaf and Franklin, Michael J and Shenker, Scott and Stoica, Ion},
  booktitle={2nd USENIX Workshop on Hot Topics in Cloud Computing (HotCloud 10)},
  year={2010}
}

@misc{kubernetesPodDocs,
  title = {{kubernets.io} Concepts/Pods},
  howpublished = {\url{https://kubernetes.io/docs/concepts/workloads/pods/}},
  note = {Accessed: 2022-03-09}
}


@misc{rbacProxy,
  title = {{github.com} brancz/kube-rbac-proxy},
  howpublished = {\url{https://github.com/brancz/kube-rbac-proxy}},
  note = {Accessed: 2022-03-09}
}

@misc{kubernetesSchedulerDocs,
  author       = {},
  howpublished = {\url{https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/}},
  title = {{kubernets.io} Concepts/Pods},
  note = {Accessed: 2022-03-09}
}

@misc{SparkOperator,
  author       = {},
  howpublished = {\url{https://github.com/GoogleCloudPlatform/spark-on-k8s-operator}},
  title = {{GitHub.com} GCP/Spark-On-K8s-Operator},
  note = {Accessed: 2022-03-10}
}

@misc{FlinkOperator,
  author       = {},
  howpublished = {\url{https://github.com/spotify/flink-on-k8s-operator}},
  title = {{GitHub.com} Spotify/Flink-On-K8s-Operator},
  note = {Accessed: 2022-03-10}
}

@misc{DataBricks,
  author       = {},
  howpublished = {\url{https://databricks.com/blog/2014/03/20/apache-spark-a-delight-for-developers.html}},
  title = {{databricks.com} Blog Apache Spark: A Delight for Developers},
  note = {Accessed: 2022-03-17}
}

@article{dean2008mapreduce,
  title={MapReduce: simplified data processing on large clusters},
  author={Dean, Jeffrey and Ghemawat, Sanjay},
  journal={Communications of the ACM},
  volume={51},
  number={1},
  pages={107--113},
  year={2008},
  publisher={ACM New York, NY, USA}
}




@InProceedings{10.1007/978-3-030-96498-6_18,
author="Misale, Claudia
and Milroy, Daniel J.
and Gutierrez, Carlos Eduardo Arango
and Drocco, Maurizio
and Herbein, Stephen
and Ahn, Dong H.
and Kaiser, Zvonko
and Park, Yoonho",
editor="Nichols, Jeffrey
and Maccabe, Arthur `Barney'
and Nutaro, James
and Pophale, Swaroop
and Devineni, Pravallika
and Ahearn, Theresa
and Verastegui, Becky",
title="Towards Standard Kubernetes Scheduling Interfaces for Converged Computing",
booktitle="Driving Scientific and Engineering Discoveries Through the Integration of Experiment, Big Data, and Modeling and Simulation",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="310--326",
abstract="High performance computing (HPC) and cloud technologies are increasingly coupled to accelerate the convergence of traditional HPC with new simulation, data analysis, machine-learning, and artificial intelligence approaches. While the HPC+cloud paradigm, or converged computing, is ushering in new scientific discoveries with unprecedented levels of workflow automation, several key mismatches between HPC and cloud technologies still preclude this paradigm from realizing its full potential. In this paper, we present a joint effort between IBM Research, Lawrence Livermore National Laboratory (LLNL), and Red Hat to address the mismatches and to bring full HPC scheduling awareness into Kubernetes, the de facto container orchestrator for cloud-native applications, which is being increasingly adopted as a key converged-computing enabler. We found Kubernetes lacking of interfaces to enable the full spectrum of converged-computing use cases in the following three areas: (A) an interface to enable HPC batch-job scheduling (e.g., locality-aware node selection), (B) an interface to enable HPC workloads or task-level scheduling, and (C) a resource co-management interface to allow HPC resource managers and Kubernetes to co-manage a resource set. We detail our methodology and present our results, whereby the advanced graph-based scheduler Fluxion -- part of the open-source Flux scheduling framework -- is integrated as a Kubernetes scheduler plug-in, KubeFlux. Our initial performance study shows that KubeFlux exhibits similar performance (up to measurement precision) to the default scheduler, despite KubeFlux's considerably more sophisticated scheduling capabilities.",
isbn="978-3-030-96498-6"
}

