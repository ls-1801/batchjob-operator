The intended use-case for the Interface is the development of new scheduling algorithms. Arbitrary-sized Testbeds can span across a single cluster controlled by Kubernetes. Kubernetes has been shown to scale with large clusters, and with Kubernetes v1.23 clusters with up to 5000 nodes are supported[@kubernetesLargeCluster]. During the prototype development, a cluster with only a few nodes was used, thus leaving the scalability of the Extern-Scheduler-Interface in an uncertain state. It is unlikely for a single Testbed to overwhelm the operator due to limiting the access to *Testbeds* to a single *Scheduling*. Furthermore, Batch Jobs are rather long-living and do not require much maintenance (at least from the Interfaces perspective). However, there are currently no limitations on how many *Testbeds* may exist in the same cluster, and thus the Interface may be overwhelmed with too many concurrent *Testbeds*. Currently, the Interface has no leader election mechanism and therefore cannot scale horizontally. Using multiple operators in different namespaces does not require a leader election mechanism but would require refining the use of namespaces inside the Interface.

A Namespace in Kubernetes is the mechanism that isolates groups of resources within the same cluster. In a shared cluster, it is commonly used to not interfere with other users without revolving back to statically partitioned clusters and lose the benefit of resource sharing. The current implementation only supports limited use of namespaces since it was developed on a private cluster and could use the "default" namespace for all its components. The Integration Tests use a different namespace, and the Interface is not tied to a specific namespace. However, for now, all components run inside the same namespace. This leaves the question if actions done by the operator should be limited to a single namespace or if it should be able to interact with resources in every namespace. Furthermore, this question is not just limited to the operators, which are part of the prototype but also application-specific operators. The Spark-Operator currently allows both options, either watch all namespaces or limit the scope to only a single namespace. 

The Interface currently does not expose the namespace to the External-Scheduler, because every resource must be inside the Interfaces namespace. This further promotes limiting the scope per instance of the Interface to a single namespace and thus limiting required privileges. However, some actions required by the Interface need access to resources across all namespaces. First of all, the *Testbed Operator*requires access to the nodes (not namespaced) and requires access to all pods running in every namespace to calculate the available resources before creating a *Testbed*. The issue with Nodes not being limited by a namespace is that multiple *Testbeds* may use the same node, which would undoubtedly cause trouble with the current implementation.

The last limitation that would prevent the Interface from being used in a *production*ish environment is the lack of Security. Both in terms of malicious Users (or just forgetful), who can reserve cluster resources with a Testbed, and accessing the cluster from outside via the API Endpoints, without proper authentication and authorization. The first issue is hard to circumvent because a malicious user could do that anyway, and Kubernetes Resource Quotas[@kubernetesResourceQuotas] can limit the resources per namespace, but they would also limit a non-malicious User. Automatically resizing a Testbed if it is not in use for a while could prevent unnecessary reservation of resources and defeat the purpose of reserving resources for an External-Scheduler. The Second issue requires API-Endpoints not to be publicly available. During development, the Interface was accessed using port-forwarding, which requires authorization to the Kubernetes cluster. For future work relying on port-forwarding seems plausible. However, creating an actual External-Scheduler down the line would require proper Ingress configuration to the cluster and a security model. 
