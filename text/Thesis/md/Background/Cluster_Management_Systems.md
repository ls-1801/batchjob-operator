In HPC Super Computers are commonly build with expansive hardwares, and tightly coupled Operating Systems, that allows parallel applications to take advantage of resources available on the super computer. Super Computers are carefully planned with software designed for specific purposes, this usually meant that multiple applications running on a super computers, would have resources statically partitioned among them. Coarse grain partitioning introduces inefficient use of hardware, once part of the statically partitioned resources are no longer in use.

![Static Partitioning](graphics/static_partitioning.png){width=25%, height=25%}

With the emerge of Distributed Dataflow Applications, like Hadoop MapReduce[@dittrich2012efficient] the need for a more fine grain partitioning of cluster resources came along. The move away from Super Computers to cheap commodity hardware, meant that cluster can be scaled up or down easily, where previously careful planning was required. Managing a dynamic system of potential thousands of Nodes, can not be done in a manual fashion. A Cluster Resource Manager is required, to build the abstraction of a single cohesive cluster, that can be tasked with jobs.

Hadoop is one of Many Open-Source MapReduce implementations. Cluster computing using commodity hardware was driven by the need to keep up with the explosion of data.
The Initial Version of Hadoop was focused on Running MapReduce Jobs to process a Web Crawl [@10.1145/2523616.2523633]. Despite the initial focus, Hadoop was widely adopted evolved to a state where it was no longer used with its initial target in mind. Wide adoptions have shown some of the weaknesses in Hadoops architecture:
- Tight Coupling between the MapReduce Programming model and Cluster Management
- Centralized Handling of Jobs will prevent Hadoop from Scaling

![Dynamic Partitioning](graphics/dynamic_partitioning.png){width=25%, height=25%}

The tight coupling leads Hadoop users with different applications to abuse the MapReduce Programming model to benefit from cluster management and be left with a suboptimal solution. A typical pattern was to submit 'map-only' jobs that act as arbitrary software running on top of the resource manager. [@10.1145/2523616.2523633]
The other solution was to create new Frameworks. This caused the invention of many frameworks that aim to solve distributed computation on a cluster for many different kinds of applications [@hindman2011mesos].
Frameworks tend to be strongly tailored to simplify solving specific problems on a cluster of computers and thus speed up the exploration of data. It was expected for many more frameworks to be created, as none of them will offer an optimal solution for all applications.[@hindman2011mesos]

Initially, Frameworks like MapReduce created and managed the cluster, which only allowed a single Application across many machinesâ€”running only a single application across a cluster of Nodes led to underutilization of the cluster's resources. The Next generation of Hadoop allowed it to build ad-hoc clusters, using Torque [@1558641] and Maui, on a shared pool of hardware. Hadoop on Demand (HoD) allowed users to submit their jobs to the cluster, estimating how many machines are required to fulfill the job. Torque would enqueue the job and reserve enough machines once they become available. Torque/Maui would then start the Hadoop Master and the Slaves, subsequently spawn the Task and JobTracker that make up a MapReduce Application. Once all Tasks are finished, the acquired machines are released back to the shared resource pool.
This can create potentially wasteful scenarios, where only a single Reduce Task is left, but many machines are still reserved for the cluster. Usually, Resources requirements are overestimated and thus leaves many clusters resources unused.

With the Introduction of Hadoop 3, the MapReduce Framework was split into the dedicated Resource Manager YARN and MapReduce itself. Now MapReduce was no longer running across a Cluster, but it was running on top of YARN, who manages the cluster beneath. This allows MapReduce to run multiple Jobs across the same YARN Cluster, but more importantly, it also allows other Frameworks to run on top of YARN. This moved a lot of complexity away from MapReduce and allowed the framework to only specialize on the MapReduce Programming Model rather than managing a cluster. This finally allows different Programming Models for Machine Learning tasks that tend to perform worse on the MapReduce Programming model [@zaharia2010spark] to run on top of the same cluster as other MapReduce Jobs.

Using the ResourceManager YARN allows for a more fine-grain partitioning of the cluster resources. Previously a static partitioning of the clusters resources was done to ensure that a specific application could use a particular number of machines. Many applications can significantly benefit from being scaled out across the cluster rather than being limited to only a few machines.
 - Fault tolerance: Frameworks use replication to be resilient in the case of machine failure. Having many of the replicas across a small number of machines defeats the purpose
 - Resource Utilization: Frameworks can scale dynamically and thus use resources that are not currently used by other Applications. This allows applications to scale out rapidly once new Nodes become available
 - Data Locality: Usually, the data to work on is shared across a cluster. Many applications are likely to work on the same set of data. Having applications sitting on only a few Nodes, but the data to be shared across the complete cluster leads to a lousy data locality. Many unnecessary I/O needs to be performed to move data across the cluster.

Fine-grain partitioning can be achieved using containerization, where previously applications were deployed in VMs, which would host a full Operating System. Many containers can be deployed on the same VM (or physical machine) and share the same Operating System Kernel. The hosting Operating makes sure Applications running inside containers are isolated by limiting their resources and access to the underlying Operating System.

Before Hadoop 3 with YARN was published, an Alternative Cluster Manager Mesos was publicized. Like YARN, Mesos allowed a more fine granular sharing of resources using containerization.
The key difference between YARN and Mesos is how resources are scheduled to frameworks running inside the cluster. YARN offers a resource request mechanism, where applications can request the resources they want to use (*TODO: fact check*), and Mesos, on the other hand, offers frameworks resources that they can use. This allows frameworks to decide better which of the resources may be more beneficial. This enables Mesos to pass the resource-intensive task of online scheduling to the frameworks and improve its scalability.

Kubernetes[@burns2016borg] was initially developed by Google and released after multiple years of intern usage. Kubernetes was quickly adopted and has become the defacto standard for managing a cluster of machines.
Kubernetes offers a descriptive way of managing resources in a cluster where manifests describing the cluster's desired state are stored inside a distributed key-value store[@grzesik2019evaluation] etcd. Controllers are running inside the cluster to monitor these manifests and do the required actions to bring the cluster into the desired state. 
Working with manifest abstracts away many of the problems that arise when deploying Applications to a cluster. Usually, an Operations Team was required to manage applications across the cluster. 
With Kubernetes offering the required building blocks and the mechanism of a control-loop, the operator pattern in combination with Custom Resource Definitions is commonly used to extend Kubernetes functionalities. Where Hadoop's YARN was focusing on Distributed Dataflow Applications, Kubernetes enables developers of all kind of Applications the scalability and resilience of the Cloud.

