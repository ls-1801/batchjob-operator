Distributed Dataflow Processing aims to solve the problem of analyzing large quantities of data. In the last years, the amount of generated data has exploded. This creates a problem where single machines can no longer analyze the data in a meaningful time. While the Big Data Processing frameworks still work on single machines, computation is usually distributed across many processes running on hundreds of machines to analyze the data in an acceptable time.

Analyzing data on a single machine is usually limited by the resources available on a single machine. Unfortunately, increasing the resources of a single machine is either not feasible from a cost standpoint or simply impossible. There is only a limited amount of processor time, memory, and I/O available. Cheap commodity hardware allows a cluster to bypass the limitations of a single machine, scaling to a point where the cluster can keep up with the generated data and once again analyze data in a meaningful time frame. 

Dealing with distributed systems is a complex topic in itself. Many assumptions that could be made in a single process context are no longer valid. Scaling to more machines increases the probability of failures. Distributed Systems need to be designed to be resilient against hardware-failures, network outages/partitions and are expected to recover from said failures. Having a single loss resulting in no or an invalid result will not scale to systems of hundreds of machines, where it is unlikely to not encounter a single failure during execution.

Scaling computational resources beyond the limitation of single machines is not a new problem. High-Performance-Computing (HPC), like supercomputers, are already commonly used for computational heavy use cases.
Distributed Dataflow Applications initially used the static partitioning approach and evolved with the advance of cloud computing and cluster management systems to scale more dynamically, depending on the users' needs. Distributed Dataflow Applications may be classically categorized as HPC applications bringing high performance and efficiency due to sophisticated scheduling, where the cloud offers resiliency, elasticity, portability, and manageability.[@10.1007/978-3-030-96498-6_18]

Distributed Dataflow frameworks can be put into two categories, although many fall in both categories: Batch Processing and Stream Processing. In Batch Processing, data size is usually known in advance, whereas Stream Processing expects new data to be streamed in from different sources during the runtime. Batch Processing jobs will complete their calculation eventually, and Stream Processing may run for an infinite time.

![Example of Spark DAG [@DataBricks]](graphics/MapReduceDAG.pdf)

Internally, Big Data Processing frameworks build a directed acyclic graph (DAG) of stages required for the analysis. Stages are critical for saving intermediate results, to resume after failure, and are usually steps during the analysis, where data moving across processes is required. A commonly used programming model is the MapReduce Programming model[@dean2008mapreduce], where stages can be generalized in Map and Reduce operations.

Map operations can be performed on many machines in parallel, without further knowledge about the complete data sets, like extracting the username from an application-log-line. Reduce operations require moving data around the cluster. These are usually used to aggregate data, like grouping by a key, summing, or counting. 

Datasets that the Distributed Processing frameworks analyze are usually in the range of terabytes which is multiple magnitudes higher than the amount of memory that each machine has available. partitioning of the data is required due to the limitations of each single machine, while persistent storage, like hard-drives, might be closer to the extent of BigData, computation will quickly become limited by the amount of I/O a single machine can perform.  Commonly a Distributed Dataflow Application is combined with a distributed file system (like HDFS[@borthakur2008hdfs]) that spans a shared filesystem across many machine. In a distributed filesystem, the filesystem is usually broken down into smaller chunks, where every machine holds a few chunks of the complete filesystem as part of their local filesystem. If data from the distributed filesystem is not local to the machine' filesystem, data will be moved across the network. Replicating chunks increases fault tolerance and thus prevents the loss of data but also requires synchronization across replicated chunks once they get updated. Data-Locality describes how close the data is, which the current task requires. If a task has good data-locality on a machine, no data movement is required.

The user of Big Data Processing frameworks is usually not required to think about how an efficient partition of the data across many processes may be accomplished. Frameworks are designed in a way where they can efficiently distribute a large amount of work across many machines.